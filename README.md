# YOLOV5 V6.0çŸ¥è¯†è’¸é¦

ä¸€äº›ç¢ç¢å¿µï¼š 3050Tiçš„ç®—åŠ›å¤ªå·®äº†ï¼Œyolov5sçš„ batchæœ€å¤§è®¾ä¸º8ï¼Œyolov5mçš„ batchæœ€å¤§è®¾ä¸º4ã€‚æ— è¯­å­

--------------
### è°ƒæ•´ä¸€: å…³é—­äº† **wandb** 
 1. `utils/loggers/wandb/wandb_utils.py`
```python
try:
    import wandb

    assert hasattr(wandb, '__version__')  # verify package import not local dir
except (ImportError, AssertionError):
    wandb = None

# ä¿®æ”¹ä¸º
wandb = None
```
 2. `utils/loggers/__init__.py`
```python
try:
    import wandb

    assert hasattr(wandb, '__version__')  # verify package import not local dir
    if pkg.parse_version(wandb.__version__) >= pkg.parse_version('0.12.2') and RANK in [0, -1]:
        try:
            wandb_login_success = wandb.login(timeout=30)
        except wandb.errors.UsageError:  # known non-TTY terminal issue
            wandb_login_success = False
        if not wandb_login_success:
            wandb = None
except (ImportError, AssertionError):
    wandb = None

# ä¿®æ”¹ä¸º
wandb = None
```
### è°ƒæ•´äºŒ: åœ¨train.pyçš„åŸºç¡€ä¸Šå¢åŠ äº†kdæ¡†æ¶ï¼Œåä¸ºï¼štrain_kd.py
 1. å¢åŠ é»˜è®¤å‚æ•°çš„è®¾å®š
```python
# æ˜¯å¦ä½¿ç”¨çŸ¥è¯†è’¸é¦
parser.add_argument('--kd', action='store_true', default=True, help='cache images for faster training')
# é¢„è®­ç»ƒå¥½çš„æ•™å¸ˆç½‘ç»œæ¨¡å‹æƒé‡æ–‡ä»¶åœ°å€
parser.add_argument('--teacher_weight', type=str, default=ROOT / 'runs/weights/YOLOV5M-NO/weights/best.pt',
                        help='initial teacher_weight path')
# ä½¿ç”¨ä½•ç§æŸå¤±å‡½æ•°è®¡ç®—è’¸é¦æŸå¤±
parser.add_argument('--kd_loss_selected', type=str, default='l2', help='using kl/l2 loss in distillation')
# å¦‚æœä½¿ç”¨çš„klè®¡ç®—æŸå¤±ï¼Œåˆ™éœ€è¦æŒ‡å®šæ¸©åº¦temperature
parser.add_argument('--temperature', type=int, default=20, help='temperature in distilling training')
```
 2. åœ¨è®­ç»ƒæµç¨‹ä¸­æ·»åŠ kd
```python
# åœ¨ä½¿ç”¨çš„hyp.yamlæ–‡ä»¶ä¸­æ·»åŠ  è®¡ç®—kdæŸå¤±çš„æ—¶å€™ä¼šç”¨åˆ°çš„å‚æ•°
giou: 0.05
kd: 1.0
```
```python
    # åŠ è½½æ•™å¸ˆæ¨¡å‹
    if opt.kd:
        print("load teacher-model from", opt.teacher_weight)
        teacher_model = torch.load(opt.teacher_weight)
        if teacher_model.get("model", None) is not None:
            teacher_model = teacher_model["model"]
        teacher_model.to(device)
        teacher_model.float()
        teacher_model.train()
```
```python
    # ç”¨äºä¿å­˜ kdæŸå¤±å€¼ 
    mkdloss = torch.zeros(1, device=device)  # mean kd_losses
    matloss = torch.zeros(1, device=device)  # mean at_losses
```
```python
    # å‰å‘ä¼ æ’­ï¼Œä¸æ›´æ–°æ•™å¸ˆç½‘ç»œçš„å‚æ•°
    if opt.kd:
        with torch.no_grad():
            teacher_pred = teacher_model(imgs)
```
```python
    # å°†kdæŸå¤±åŠ å…¥åˆ°æ€»æŸå¤±ï¼Œä»¥ä¾¿åå‘ä¼ æ’­ï¼Œæ›´æ–°å­¦ç”Ÿç½‘ç»œå‚æ•°
    # Forward
    with amp.autocast(enabled=cuda):
        s_f = get_feas_by_hook(model) # è·å–å­¦ç”Ÿç½‘ç»œä¸­é—´å±‚ç‰¹å¾
        pred = model(imgs)  # forward

        if opt.kd:
            atloss = torch.zeros(1, device=device)
            with torch.no_grad():
                t_f = get_t_feas_by_hook(teacher_model) # è·å–æ•™å¸ˆç½‘ç»œä¸­é—´å±‚ç‰¹å¾
                teacher_pred = teacher_model(imgs)

            for i in range(len(t_f)):
                # è®¡ç®—ATloss
                atloss += at_loss(s_f[i].fea, t_f[i].fea)
        
        del s_f, t_f

        loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size
        # kd
        if opt.kd:
            kdloss = compute_kd_output_loss(
                pred, teacher_pred, model, opt.kd_loss_selected, opt.temperature)
        else:
            kdloss = 0
        del teacher_pred
        loss += opt.alpha * kdloss + opt.beta * atloss
        loss_items[-1] = loss

        if RANK != -1:
            loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode
        if opt.quad:
            loss *= 4.
```
```python
    # æ›´æ–° mkdloss æ—¥å¿—
    mkdloss = (mkdloss * i + kdloss) / (i + 1)  # update mean losses
    matloss = (matloss * i + atloss) / (i + 1)  # update mean losses
    pbar.set_description(('%10s' * 2 + '%10.4g' * 7) %
                         (f'{epoch}/{epochs - 1}', mem, *mloss, mkdloss, matloss, targets.shape[0], imgs.shape[-1]))

```

 3. æ–°å¢æŸå¤±å‡½æ•°çš„è®¡ç®—
```python
def compute_kd_output_loss(pred, teacher_pred, model, kd_loss_selected="l2", temperature=20, reg_norm=None):
    t_ft = torch.cuda.FloatTensor if teacher_pred[0].is_cuda else torch.Tensor
    t_lcls, t_lbox, t_lobj = t_ft([0]), t_ft([0]), t_ft([0])
    h = model.hyp  # hyperparameters
    red = 'mean'  # Loss reduction (sum or mean)
    if red != "mean":
        raise NotImplementedError(
            "reduction must be mean in distillation mode!")

    KDboxLoss = nn.MSELoss(reduction="none")
    if kd_loss_selected == "l2":
        KDclsLoss = nn.MSELoss(reduction="none")
    elif kd_loss_selected == "kl":
        KDclsLoss = nn.KLDivLoss(reduction="none")
    else:
        KDclsLoss = nn.BCEWithLogitsLoss(reduction="none")
    KDobjLoss = nn.MSELoss(reduction="none")
    # per output
    for i, pi in enumerate(pred):  # layer index, layer predictions
        # t_pi  -->  torch.Size([16, 3, 80, 80, 25])
        t_pi = teacher_pred[i]
        # t_obj_scale  --> torch.Size([16, 3, 80, 80])
        t_obj_scale = t_pi[..., 4].sigmoid()
        # zero = torch.zeros_like(t_obj_scale)
        # t_obj_scale = torch.where(t_obj_scale < 0.5, zero, t_obj_scale)

        # BBox
        # repeat æ˜¯æ²¿ç€åŸæ¥çš„ç»´åº¦åšå¤åˆ¶  torch.Size([16, 3, 80, 80, 4])
        b_obj_scale = t_obj_scale.unsqueeze(-1).repeat(1, 1, 1, 1, 4)
        if not reg_norm:
            t_lbox += torch.mean(KDboxLoss(pi[..., :4], t_pi[..., :4]) * b_obj_scale)
        else:
            wh_norm_scale = reg_norm[i].unsqueeze(0).unsqueeze(-2).unsqueeze(-2)
            # pxy
            t_lbox += torch.mean(KDboxLoss(pi[..., :2].sigmoid(), t_pi[..., :2].sigmoid()) * b_obj_scale)
            # pwh
            t_lbox += torch.mean(
                KDboxLoss(pi[..., 2:4].sigmoid(), t_pi[..., 2:4].sigmoid() * wh_norm_scale) * b_obj_scale)

        # Class
        if model.nc > 1:  # cls loss (only if multiple classes)
            c_obj_scale = t_obj_scale.unsqueeze(-1).repeat(1, 1, 1, 1, model.nc)
            if kd_loss_selected == "kl":
                kl_loss = KDclsLoss(F.log_softmax(pi[..., 5:] / temperature, dim=-1),
                                    F.softmax(t_pi[..., 5:] / temperature, dim=-1)) * (temperature * temperature)
                t_lcls += torch.mean(kl_loss * c_obj_scale)
            else:
                t_lcls += torch.mean(KDclsLoss(pi[..., 5:], t_pi[..., 5:]) * c_obj_scale)

        t_lobj += torch.mean(KDobjLoss(pi[..., 4], t_pi[..., 4]) * t_obj_scale)
    t_lbox *= h['giou'] * h['kd']
    t_lobj *= h['obj'] * h['kd']
    t_lcls *= h['cls'] * h['kd']
    bs = pred[0].shape[0]  # batch size
    mkdloss = (t_lobj + t_lbox + t_lcls) * bs
    return mkdloss


def at(x):
    # mc = x.mean(3, keepdim=True).mean(2, keepdim=True)
    # Mc = mc.sigmoid()
    # x = torch.mul(Mc, x)
    return F.normalize(x.pow(2).mean(1).view(x.size(0), -1))


def at_loss(x, y):
    return (at(x) - at(y)).pow(2).mean()
```
### è°ƒæ•´äºŒ: åœ¨å¤§æ¨¡å‹çš„åŸºç¡€ä¸Šä¿®æ”¹ç½‘ç»œç»“æ„ï¼ˆå¦‚Yolov5m-CAï¼‰ 
 1. æ·»åŠ æ³¨æ„åŠ›æ¨¡å—ï¼ˆæ·»åŠ åˆ° models/common.py ä¸­ï¼‰
```python
# SEæ¨¡å—
class SELayer(nn.Module):
    def __init__(self, c1, r=16):
        super(SELayer, self).__init__()
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.l1 = nn.Linear(c1, c1 // r, bias=False)
        self.relu = nn.ReLU(inplace=True)
        self.l2 = nn.Linear(c1 // r, c1, bias=False)
        self.sig = nn.Sigmoid()

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avgpool(x).view(b, c)
        y = self.l1(y)
        y = self.relu(y)
        y = self.l2(y)
        y = self.sig(y)
        y = y.view(b, c, 1, 1)
        return x * y.expand_as(x)
```
```python
# CBAMæ¨¡å—
# æ ‡å‡†å·ç§¯å±‚ + CBAM
class ASCBAM(nn.Module):
    # Standard convolution
    def __init__(self, c1, ratio):  # ch_in, ch_out, kernel, stride, padding, groups
        super(ASCBAM, self).__init__()
        self.ca = ChannelAttention(c1, ratio, AS=False)
        self.sa = SpatialAttention(c1, AS=False)

    def forward(self, x):
        x = self.sa(self.ca(x))
        return x



# add CBAM
class SpatialAttention(nn.Module):
    def __init__(self, in_planes, kernel_size=7, AS=False):
        super(SpatialAttention, self).__init__()
        self.conv3 = nn.Conv2d(in_planes, in_planes, 3, padding=1)
        self.conv1_1 = nn.Conv2d(in_planes, in_planes, 1)
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=3, bias=False)  # concatå®Œchannelç»´åº¦ä¸º2
        self.sigmoid = nn.Sigmoid()
        self.AS = AS

    def forward(self, x):
        if self.AS:
            avg_out = torch.mean(x, dim=1, keepdim=True)
            max_out = self.sigmoid(self.conv1_1(self.conv3(x)))
            max_out1, _ = torch.max(max_out, dim=1, keepdim=True)
            y = torch.cat([avg_out, max_out1], dim=1)  # æ²¿ç€channelç»´åº¦concatä¸€å—
        else:
            avg_out = torch.mean(x, dim=1, keepdim=True)  # æ²¿ç€channel ç»´åº¦è®¡ç®—å‡å€¼å’Œæœ€å¤§å€¼
            max_out1, _ = torch.max(x, dim=1, keepdim=True)
            y = torch.cat([avg_out, max_out1], dim=1)  # æ²¿ç€channelç»´åº¦concatä¸€å—
        y = self.conv1(y)
        y = self.sigmoid(y)
        return x * y


class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16, AS=False):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.max_pool_3d = nn.AdaptiveAvgPool3d((in_planes, 1, 1))

        self.conv3 = nn.Conv2d(in_planes, in_planes * 2, 3)
        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)
        self.sigmoid = nn.Sigmoid()
        self.AS = AS

    def forward(self, x):
        if self.AS:
            max_out = self.fc2(self.relu1(self.fc1(self.max_pool_3d(self.conv3(x)))))
            avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        else:
            avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
            max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return x * self.sigmoid(out)
```
```python
# CAæ¨¡å—
class CABlock(nn.Module):
    def __init__(self, inp, oup, reduction=32):
        super(CABlock, self).__init__()
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        mip = max(8, inp // reduction)
        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)
        self.bn1 = nn.BatchNorm2d(mip)
        self.act = h_swish()
        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)
        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)

    def forward(self, x):
        identity = x
        n, c, h, w = x.size()
        x_h = self.pool_h(x)
        x_w = self.pool_w(x).permute(0, 1, 3, 2)
        y = torch.cat([x_h, x_w], dim=2)
        y = self.conv1(y)
        y = self.bn1(y)
        y = self.act(y)
        x_h, x_w = torch.split(y, [h, w], dim=2)
        x_w = x_w.permute(0, 1, 3, 2)
        a_h = self.conv_h(x_h).sigmoid()
        a_w = self.conv_w(x_w).sigmoid()
        out = identity * a_w * a_h
        return out
```
 2. ä¿®æ”¹ç½‘ç»œç»“æ„ï¼ˆmodels/yolo.pyï¼‰
```python
# åœ¨ parse_model() å‡½æ•°ä¸‹ç…§ç€æ·»åŠ 
    elif m is SELayer:
        channel, re = args[0], args[1]
        channel = make_divisible(channel * gw, 8) if channel != no else channel
        args = [channel, re]
    elif m is CABlock:
        channel, re = args[0], args[1]
        channel = make_divisible(channel * gw, 8) if channel != no else channel
        args = [channel, channel, re]
    elif m is ASCBAM:
        channel, re = args[0], args[1]
        channel = make_divisible(channel * gw, 8) if channel != no else channel
        args = [channel, re]
```
 3. ä¿®æ”¹é…ç½®æ–‡ä»¶--åœ¨æ¯ä¸ªC3æ¨¡å—åé¢æ·»åŠ ATMï¼ˆå¦‚ä¿®æ”¹ models/yolov5m.yamlï¼‰
```yaml
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license

# Parameters
nc: 20  # number of classes
depth_multiple: 0.67  # model depth multiple
width_multiple: 0.75  # layer channel multiple
anchors:
  - [10,13, 16,30, 33,23]  # P3/8
  - [30,61, 62,45, 59,119]  # P4/16
  - [116,90, 156,198, 373,326]  # P5/32

# YOLOv5 v6.0 backbone
backbone:
  # [from, number, module, args]
  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2
   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
   [-1, 3, C3, [128]],    # 2
   [-1, 1, SELayer, [128, 4]], # 3
   [-1, 1, Conv, [256, 3, 2]],  # 4-P3/8
   [-1, 6, C3, [256]],
   [-1, 1, SELayer, [256, 4]], #6
   [-1, 1, Conv, [512, 3, 2]],  # 7-P4/16
   [-1, 9, C3, [512]],
   [-1, 1, SELayer, [512, 4]], # 9
   [-1, 1, Conv, [1024, 3, 2]],  # 10 -P5/32
   [-1, 3, C3, [1024]],
   [-1, 1, SELayer, [1024, 4]], #12
   [-1, 1, SPPF, [1024, 5]],  # 13
  ]

# YOLOv5 head
head:
  [[-1, 1, Conv, [512, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 9], 1, Concat, [1]],  # cat backbone P4
   [-1, 3, C3, [512, False]],  # 17
   [ -1, 1, SELayer, [512, 4] ], #18

   [-1, 1, Conv, [256, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 6], 1, Concat, [1]],  # cat backbone P3
   [-1, 3, C3, [256, False]],  # 22 (P3/8-small)
   [ -1, 1, SELayer, [256, 4] ], #23

   [-1, 1, Conv, [256, 3, 2]],
   [[-1, 19], 1, Concat, [1]],  # cat head P4
   [-1, 3, C3, [512, False]],  # 26 (P4/16-medium)
   [ -1, 1, SELayer, [512, 4] ], #27

   [-1, 1, Conv, [512, 3, 2]],
   [[-1, 14], 1, Concat, [1]],  # cat head P5
   [-1, 3, C3, [1024, False]],  # 30 (P5/32-large)
   [ -1, 1, SELayer, [1024, 4] ], #31

   [[23, 27, 31], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
  ]
```



